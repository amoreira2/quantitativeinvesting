{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WI820A_Bp_NH"
   },
   "source": [
    "# Assignment 6\n",
    "\n",
    "\n",
    "\n",
    "Instructions: This problem set should be done in a **group**.\n",
    "\n",
    "Your group was assigned at orientation and you can find it in blackboard as well.\n",
    "\n",
    "You will use this same group to do your final project.\n",
    "\n",
    "Answer each question in the designated space below.\n",
    "\n",
    "After you are done. save and upload in blackboard.\n",
    "\n",
    "Please check that you are submitting the correct file. One way to avoid mistakes is to save it with a different name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wVAN3J3Vp_NJ",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Names of your group members\n",
    "\n",
    "Please write names below\n",
    "\n",
    "* [Name]:\n",
    "* [Name]:\n",
    "* [Name]:\n",
    "* [Name]:\n",
    "* [Name]:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion Forum Assignment\n",
    "\n",
    "\n",
    "Do the Value investing assignment in the discussion forum\n",
    "\n",
    "https://edstem.org/us/courses/30665/discussion/1987699\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Preliminaries\n",
    "\n",
    "\n",
    "You will work with the same dataset as in Assigment 5.\n",
    "The dataset has address \n",
    "\n",
    "`url='https://github.com/amoreira2/Lectures/blob/main/assets/data/Assignment5.xlsx?raw=true'`\n",
    "\n",
    "\n",
    "Follow the steps of Assigment 5 questions 1, 2, 3, 5, 6 (you can literally copy and paste your code):\n",
    "\n",
    "- Import pandas, numpy, matplotlib, and load the data set.\n",
    "- Import the datasets of industry returns and risk free rate.\n",
    "- Parse the date.\n",
    "- Set the index.\n",
    "- Drop missing observations.\n",
    "- Construct a dataframe with only excess returns.\n",
    "- Call this dataframe with the 49 excess returns time series `df`.\n",
    "\n",
    "And call `df.head()` so you can see things were imported correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Expected excess return estimation\n",
    "\n",
    "Compute the sample mean as the estimators for the expected excess returns of the 49 assets. \n",
    "\n",
    "Call this `ERe`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Expected excess return uncertainty\n",
    "\n",
    "We will now construct an estimator for the amount of uncertainty in our sample mean estimator. If we assume that each individual asset is uncorrelated over time (not terrible assumption), then the variance of the mean is\n",
    "\n",
    "$$var(\\bar{r_i}) = var(\\frac{\\sum_t^T r_{i,t}}{T})=\\frac{\\sum_t^T var( r_{i,t})}{T^2} = \\frac{var(r_{i,t})}{T}$$\n",
    "\n",
    "So all you need is the sample size (T) and the variance of each asset to obtain the varaince of our estimator.\n",
    "\n",
    "Please use this formula to compute the **STANDARD DEVIATION** of sample average estimator of each 49 asset. Call this `ERe_se`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Constructing a confidence interval for the expected excess return 1\n",
    "\n",
    "\n",
    "We will now want to construct the 95% confidence interval for our estimator. The interval is such that it contains the true mean 95% of the time.\n",
    "\n",
    "The way to do this is to use the normal distribution CDF to figure out the threshold that leaves only 2.5% probability at the each side of the tails.\n",
    "\n",
    "Why 2.5% and not 5%? Because it is symmetric, so there is 2.5% probability in the left tail and 2.5 % in the right tail so overall there is only 5% probability that the expected return is out of the interval. Thus there is 95% probability that it is in the interval.\n",
    "\n",
    "To find this threshold you will start by importing  the stats library from the scipy package with \n",
    "\n",
    "`from scipy import stats`.\n",
    "\n",
    "You will then get the standard normal distribution with\n",
    "\n",
    "`sn=stats.norm(0,1)`,\n",
    "\n",
    "where 0 is the mean and 1 is the standard deviation\n",
    "\n",
    "and then get the threshold by using inverse cumulative distribution function for the appropriate `prob_value` to create a 95% CI. (see discussion above)\n",
    "\n",
    "`threshold=sn.isf(prob_value)`\n",
    "\n",
    "Make sure that this threshold is positive (if you got from the left tail, you will have to take the absolute value or just get from the right tail).\n",
    "\n",
    "Make sure you did things correctly by calling `print(threshold)`.\n",
    "\n",
    "You can always check in the normal table that you did things correctly https://en.wikipedia.org/wiki/Normal_distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Constructing a confidence interval for the expected excess return 2\n",
    "\n",
    "Armed with these threshold you can construct the interval as follows\n",
    "\n",
    "$$[\\bar{r}-threshold\\times\\sigma(\\bar{r}),\\bar{r}+threshold\\times\\sigma(\\bar{r})]$$\n",
    "\n",
    "In practice we will first construct the lower bound of the interval\n",
    "\n",
    "$\\bar{r}-threshold\\times\\sigma(\\bar{r})$\n",
    "\n",
    "and compute the upper bound symmetrically.\n",
    "\n",
    "Create an empty dataframe which has the names of industries as index and 'lower' and 'upper' as column names. Name it `ERe_ci`.\n",
    "\n",
    "Store the lower and upper bounds of CI into this dataframe. \n",
    "\n",
    "Call `ERe_ci.head()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compute the tangency portfolio weights for a portfolio with annualized volatility of 10%\n",
    "\n",
    "Store these in a dataframe whose rows are the names of the assets and the first column has the label 'mve_data'.\n",
    "\n",
    "Name this data frame `Weights`.\n",
    "\n",
    "`print(Weights)`\n",
    "\n",
    "TIP: You did this in Assigment 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sensitivity to uncertainty of the Tangent portfolio calculation 1\n",
    "\n",
    "\n",
    "Now we will compute the tangency portfolio but using a slightly different estimate for the mean.\n",
    "\n",
    "Instead of using the sample mean for each asset, first we will pick one asset, like `Hlth`. \n",
    "\n",
    "Change its mean to be its lower bound of CI in Exercise 5 and then recalculate the tangency portfolio weights.\n",
    "\n",
    "Store this in dataframe Weights with the column name `mve_Hlth-1.95`.\n",
    "\n",
    "Create another column with weights computed from the perturbation in which its mean is changed to be the upper bound of CI, label this column `mve_Hlth+1.95 `. \n",
    "\n",
    "Do a bar plot of these three sets of weights using Weights.plot.bar().\n",
    "\n",
    "Tip: you might want to create a copy of your ERe estimator before you do the perturbation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Discuss what you notice in the plot above. How much do the weights change? Which assets are impacted? Why?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance impact of estimation uncertainty\n",
    "\n",
    "\n",
    "Your Weight dataframe has 3 different weight schemes. \n",
    "\n",
    "Compute the in-sample Sharpe Ratio for these 3 different weight schemes.\n",
    "\n",
    "Notice that you should use the **real** data (e.g. `df`,`ERe`,`CovRe` ) to compute the Sharpe ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Discuss the results that you obtained above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Reporduce analysis of Exercises 7-9 for all assets\n",
    "\n",
    "Use a for loop to loop through the 49 portfolios and create the \"perturbed\" weights and the Sharpe Ratio of the perturbed weights. \n",
    "\n",
    "Then record for each asset the average drop in the Sharpe Ratio  associated with the perturbation in the tangency portfolio weights. \n",
    "\n",
    "Record in a dataframe named `dSR` (difference in SR):\n",
    "\n",
    "$$dSR[asset]=\\frac{1}{2}\\frac{SR(asset+1.95)+SR(asset-1.95)}{SR(data)}$$\n",
    "\n",
    "where SR(asset+1.95) and SR(asset-1.95) were the Sharpe ratios obtained when you perturb the expected excess return of that asset to the upper and lower bound of the CI.\n",
    "\n",
    "`dSR` should be a dataframe with industry names as index and a column, called `SR_change`, containing the results of the calculation from the expression above.\n",
    "\n",
    "Note that all you need to do here is to get the code you developed above and adapt it to work with a for loop.\n",
    "\n",
    "\n",
    "Do a bar plot of this Sharpe ratio change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. What do you think is the key takeaway from the analysis above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Monte Carlo 1\n",
    "\n",
    "So far our focus is on the estimation uncertainty of risk premiums. Covariance matrices also need to be estimated.\n",
    "\n",
    "You will now implement a Monte-Carlo method to evaluate the overall uncertainty in the construction of the tangency portfolio.\n",
    "\n",
    "You already have the sample estimates from the vector of expected excess returns `ERe` and the variance-covariance matrix `CovRe`  (you used those in Exercise 6).\n",
    "\n",
    "\n",
    "Now you use the function `np.random.multivariate_normal` to simulate draws from a multivariate normal distribution with vector of mean equal to `ERe` and the covariance matrix equal to `CovRe`.\n",
    "\n",
    "(TIP: type `np.random.multivariate_normal?` to see how this function works.) \n",
    "\n",
    "First write the code that draws ONE realization of returns for this set of 49 assets. So you should get a vector 49 by 1 that changes every time you run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Monte Carlo 2\n",
    "\n",
    "Now set the parameter `size` in the 'multivaraite_normal' function to draw T realizations of the 49 assets, where you set T to the number of months you have in the data set.\n",
    "\n",
    "Print the shape of your draw. This should return you a T by N matrix of returns, something with exactly the same shape as our data set. \n",
    "\n",
    "And every time you run the cell again you get a different realization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Monte Carlo 3\n",
    "\n",
    "Now copy the code above, so you have a simulated sample of monthly industry returns.\n",
    "\n",
    "Use the simulated return data and the weights in `mve_data` column of the dataframe `Weights` to construct a time-series of portfolio excess return. \n",
    "\n",
    "Compute and print its Sharpe Ratio. \n",
    "\n",
    "Every time you run this cell you should get a different Sharpe Ratio. This variation reflects the amount of overall uncertainty built in our investment strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Monte Carlo 4\n",
    "\n",
    "Now copy the code of the question above and write a foor loop around it. \n",
    "\n",
    "Loop throught this code 1000 times and each time record the resulting Sharpe Ratio.\n",
    "\n",
    "Save this in a dataframe called `MC`.\n",
    "\n",
    "Create a histogram of these Sharpe Ratios with 50 bins using the method `.hist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. What do you conclude?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Bootstrap\n",
    "\n",
    "The Monte-Carlo approach assumes that the distribution of returns is normal which is a good approximation but not literally true. It turns out that we can use another approach called Bootstrap that instead of sampling from the nomral distribution, samples from the actual data. \n",
    "\n",
    "Basically, bootstrap approach randomly draws one observation (one month of 49 industy returns in our case) from the real dataset, treats it as a new observation in the simulated sample and repeats this process until the simulated sample has needed sample size. Observations are drawn with replacement, which means we draw from the whole real dataset everytime so it is very likely that some data points are drawn more than once.  \n",
    "\n",
    "So you keep all your code from question 16, the only difference is that instead of calling 'np.random.multivariate_normal' to draw a sample, we will use the method 'sample' (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html).\n",
    "\n",
    "You will set the parameter 'frac' to 1 so it draws a distribution of excactly same size as our original sample and the parameter 'replace' to 'True' so it samples with replacement (otherwise you will get exactly the same realizations, only in different order).\n",
    "\n",
    "Now you can simply plug this realziation in your code from question 16.\n",
    "\n",
    "But save the results in a dataframe called `Boot`.\n",
    "\n",
    "Create a histogram of these Sharpe Ratios with 50 bins using the method `.hist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Compare the results in 16 and 18. Does the key takeway change? Does the distribution of the SR of our strategy change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Please explain why an investor should care about these results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Homework_2019_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
